{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lAS-gJZ5Ly7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bd8942-d1d9-4147-f7a7-db310faa34f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the most popular programming language?\n"
      ],
      "metadata": {
        "id": "TzK7xIdeMdEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file (replace 'path_to_your_file.csv' with the actual file path)\n",
        "file_path = '/content/drive/MyDrive/Affinity task 1/github_repos_with_commit_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Determine the most popular programming language\n",
        "most_popular_language = data['Language'].value_counts().idxmax()\n",
        "language_count = data['Language'].value_counts().max()\n",
        "\n",
        "print(f\"The most popular programming language is '{most_popular_language}' with {language_count} occurrences.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOGN2waoMN1V",
        "outputId": "0b0ed49e-8282-4e78-a51e-f584aa94f5a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most popular programming language is 'Python' with 514 occurrences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the repository with the most amount of languages?\n"
      ],
      "metadata": {
        "id": "pzX4hCekM_06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file (replace 'path_to_your_file.csv' with the actual file path)\n",
        "file_path = '/content/drive/MyDrive/Affinity task 1/github_repos_with_commit_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Count unique languages per repository\n",
        "repository_language_count = data.groupby('Repository')['Language'].nunique()\n",
        "\n",
        "# Find the repository with the most unique languages\n",
        "repository_with_most_languages = repository_language_count.idxmax()\n",
        "max_languages = repository_language_count.max()\n",
        "\n",
        "print(f\"The repository with the most amount of languages is '{repository_with_most_languages}' with {max_languages} unique languages.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-4jI977Mr5n",
        "outputId": "69970b05-3b55-42b9-c721-ef2b5f0b3b23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The repository with the most amount of languages is 'Pandas' with 3 unique languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is the repository with the most copies?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "44Wr6ZT5N5R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Affinity task 1/github_repos_with_commit_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Find the repository with the most forks\n",
        "repository_with_most_forks = data.loc[data['Forks'].idxmax()]\n",
        "\n",
        "repository_name = repository_with_most_forks['Repository']\n",
        "most_forks = repository_with_most_forks['Forks']\n",
        "\n",
        "print(f\"The repository with the most copies (forks) is '{repository_name}' with {most_forks} forks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrUhrJbhNogl",
        "outputId": "378520af-124b-47ca-b8dc-bc56f807a118"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The repository with the most copies (forks) is 'pandas' with 18075 forks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What are the top 10 repositories that most use the \"pandas\" library\n"
      ],
      "metadata": {
        "id": "9c_VJ2y3Og_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the CSV file (replace 'path_to_your_file.csv' with the actual file path)\n",
        "file_path = '/content/drive/MyDrive/Affinity task 1/github_repos_with_commit_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Filter repositories using Python (assuming Python repositories use \"pandas\")\n",
        "python_repos = data[data['Language'] == 'Python']\n",
        "\n",
        "# Sort by forks (as a proxy for popularity) and select the top 10\n",
        "top_10_python_repos = python_repos.sort_values('Forks', ascending=False).head(10)\n",
        "\n",
        "# Display the top 10 repositories\n",
        "print(\"Top 10 repositories that most likely use the 'pandas' library:\")\n",
        "print(top_10_python_repos[['Repository', 'Forks', 'URL']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utUJjS_BOCFk",
        "outputId": "46b28e7b-ba06-4797-82fa-0f47420d8099"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 repositories that most likely use the 'pandas' library:\n",
            "           Repository  Forks                                            URL\n",
            "0              pandas  18075           https://github.com/pandas-dev/pandas\n",
            "1   30-Days-Of-Python   8268  https://github.com/Asabeneh/30-Days-Of-Python\n",
            "12        mlcourse.ai   5674           https://github.com/Yorko/mlcourse.ai\n",
            "10                abu   3807                https://github.com/bbfamily/abu\n",
            "4            datasets   2710        https://github.com/huggingface/datasets\n",
            "5            yfinance   2475         https://github.com/ranaroussi/yfinance\n",
            "8                dask   1720                   https://github.com/dask/dask\n",
            "9     ydata-profiling   1689     https://github.com/ydataai/ydata-profiling\n",
            "2                tqdm   1364                   https://github.com/tqdm/tqdm\n",
            "6           pandas-ai   1335       https://github.com/Sinaptik-AI/pandas-ai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Who is the most active commit author? In which repository and in what language?"
      ],
      "metadata": {
        "id": "FUVvQjFH4vKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Load the CSV file into a DataFrame\n",
        "# file_path = '/content/drive/MyDrive/Affinity task 1/github_repos_with_commit_data.csv'\n",
        "# data = pd.read_csv(file_path)\n",
        "\n",
        "# # Ensure required columns are present\n",
        "# required_columns = {'author', 'repository', 'language', 'commit_count'}\n",
        "# if not required_columns.issubset(data.columns):\n",
        "#     raise ValueError(f\"Missing required columns. Ensure the CSV contains: {', '.join(required_columns)}\")\n",
        "\n",
        "# # Convert commit_count to numeric if it's not already\n",
        "# data['commit_count'] = pd.to_numeric(data['commit_count'], errors='coerce')\n",
        "\n",
        "# # Drop rows with NaN values in commit_count\n",
        "# data = data.dropna(subset=['commit_count'])\n",
        "\n",
        "# # Find the most active commit author\n",
        "# most_active = data.loc[data['commit_count'].idxmax()]\n",
        "\n",
        "# # Display the result\n",
        "# print(\"Most Active Commit Author:\")\n",
        "# print(f\"Author: {most_active['author']}\")\n",
        "# print(f\"Repository: {most_active['repository']}\")\n",
        "# print(f\"Language: {most_active['language']}\")\n",
        "# print(f\"Commit Count: {most_active['commit_count']}\")\n"
      ],
      "metadata": {
        "id": "JoRFJpimOokn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a timeline with the number of commits that the author has made per week based on the committer.time_sec field"
      ],
      "metadata": {
        "id": "w5mw1XLC4-8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import datetime as dt\n",
        "\n",
        "# # Load the CSV file into a DataFrame\n",
        "# file_path = '/content/drive/MyDrive/Affinity task 1/github_repos_with_commit_data.csv'\n",
        "# data = pd.read_csv(file_path)\n",
        "\n",
        "# # Ensure required columns are present\n",
        "# required_columns = {'author', 'repository', 'language', 'commit_count', 'committer.time_sec'}\n",
        "# if not required_columns.issubset(data.columns):\n",
        "#     raise ValueError(f\"Missing required columns. Ensure the CSV contains: {', '.join(required_columns)}\")\n",
        "\n",
        "# # Convert commit_count to numeric if it's not already\n",
        "# data['commit_count'] = pd.to_numeric(data['commit_count'], errors='coerce')\n",
        "\n",
        "# # Convert committer.time_sec to datetime\n",
        "# data['committer.time_sec'] = pd.to_datetime(data['committer.time_sec'], unit='s', errors='coerce')\n",
        "\n",
        "# # Drop rows with NaN values in commit_count or committer.time_sec\n",
        "# data = data.dropna(subset=['commit_count', 'committer.time_sec'])\n",
        "\n",
        "# # Find the most active commit author\n",
        "# most_active_author = data.loc[data['commit_count'].idxmax(), 'author']\n",
        "\n",
        "# # Filter data for the most active author\n",
        "# author_data = data[data['author'] == most_active_author]\n",
        "\n",
        "# # Create a timeline with the number of commits per week\n",
        "# author_data['week'] = author_data['committer.time_sec'].dt.to_period('W').dt.start_time\n",
        "# weekly_commits = author_data.groupby('week').size().reset_index(name='commit_count')\n",
        "\n",
        "# # Display the timeline\n",
        "# print(\"Timeline of Weekly Commits for Most Active Author:\")\n",
        "# print(weekly_commits)\n",
        "\n",
        "# # Optionally, save the timeline to a CSV file\n",
        "# output_path = '/mnt/data/weekly_commits_timeline.csv'\n",
        "# weekly_commits.to_csv(output_path, index=False)\n",
        "# print(f\"Weekly commit timeline saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "LtrJaHzEwv4I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify which was the first repository to commit code with \"pandas\" and which was the last."
      ],
      "metadata": {
        "id": "H249oxEt5Hqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import datetime as dt\n",
        "\n",
        "# # Load the CSV file into a DataFrame\n",
        "# file_path = '/content/drive/MyDrive/Affinity task 1/github_repos_with_commit_data.csv'\n",
        "# data = pd.read_csv(file_path)\n",
        "\n",
        "# # Ensure required columns are present\n",
        "# required_columns = {'author', 'repository', 'language', 'commit_count', 'committer.time_sec'}\n",
        "# if not required_columns.issubset(data.columns):\n",
        "#     raise ValueError(f\"Missing required columns. Ensure the CSV contains: {', '.join(required_columns)}\")\n",
        "\n",
        "# # Convert commit_count to numeric if it's not already\n",
        "# data['commit_count'] = pd.to_numeric(data['commit_count'], errors='coerce')\n",
        "\n",
        "# # Convert committer.time_sec to datetime\n",
        "# data['committer.time_sec'] = pd.to_datetime(data['committer.time_sec'], unit='s', errors='coerce')\n",
        "\n",
        "# # Drop rows with NaN values in commit_count or committer.time_sec\n",
        "# data = data.dropna(subset=['commit_count', 'committer.time_sec'])\n",
        "\n",
        "# # Find the most active commit author\n",
        "# most_active_author = data.loc[data['commit_count'].idxmax(), 'author']\n",
        "\n",
        "# # Filter data for the most active author\n",
        "# author_data = data[data['author'] == most_active_author]\n",
        "\n",
        "# # Create a timeline with the number of commits per week\n",
        "# author_data['week'] = author_data['committer.time_sec'].dt.to_period('W').dt.start_time\n",
        "# weekly_commits = author_data.groupby('week').size().reset_index(name='commit_count')\n",
        "\n",
        "# # Display the timeline\n",
        "# print(\"Timeline of Weekly Commits for Most Active Author:\")\n",
        "# print(weekly_commits)\n",
        "\n",
        "# # Optionally, save the timeline to a CSV file\n",
        "# output_path = '/mnt/data/weekly_commits_timeline.csv'\n",
        "# weekly_commits.to_csv(output_path, index=False)\n",
        "# print(f\"Weekly commit timeline saved to {output_path}\")\n",
        "\n",
        "# # Identify the first and last repository to commit code with \"pandas\"\n",
        "# pandas_commits = data[data['language'].str.contains(\"pandas\", case=False, na=False)]\n",
        "# first_pandas_commit = pandas_commits.loc[pandas_commits['committer.time_sec'].idxmin()]\n",
        "# last_pandas_commit = pandas_commits.loc[pandas_commits['committer.time_sec'].idxmax()]\n",
        "\n",
        "# print(\"First Repository to Commit with Pandas:\")\n",
        "# print(f\"Repository: {first_pandas_commit['repository']}, Time: {first_pandas_commit['committer.time_sec']}\")\n",
        "\n",
        "# print(\"Last Repository to Commit with Pandas:\")\n",
        "# print(f\"Repository: {last_pandas_commit['repository']}, Time: {last_pandas_commit['committer.time_sec']}\")\n"
      ],
      "metadata": {
        "id": "rvLKVSypxVsC"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}